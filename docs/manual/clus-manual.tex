\documentclass[a4paper]{report}
\usepackage{geometry}
\usepackage{graphics}
\usepackage{ifpdf}
\usepackage{makeidx}
\usepackage{fancyhdr}
\usepackage{fancyvrb}
\usepackage{amsmath, amsthm, amssymb}

\ifpdf
\pdfinfo{
  /Title    (Clus: User's Manual)
  /Author   (Jan Struyf, Bernard Zenko, Hendrik Blockeel, Saso Dzeroski)
}
\usepackage[pdftex,colorlinks=true,pdfstartview=FitV,linkcolor=black,citecolor=black,urlcolor=black]{hyperref}
\else
\usepackage{url}
\newcommand{\phantomsection}[1]{}
\fi

% geometry makes sure page size is correct in .pdf files
\geometry{a4paper,
          centering,
          textwidth = 16.5cm,
          textheight = 24.5cm
}

\renewcommand{\topfraction}{1.0}
\setcounter{topnumber}{4}
\setcounter{totalnumber}{4}
\renewcommand{\textfraction}{0.07}

\newcommand{\clus}{\textsc{Clus}}
\newcommand{\clusphy}{\textsc{Clus}-$\varphi$}
%\newcommand{\comment}[1]{{\color{red}\textbf{COMMENT:} #1}}
\newcommand{\comment}[1]{} % off

\begin{document}

\title{\clus: User's Manual}

\author{Jan Struyf, Bernard \v{Z}enko, Hendrik Blockeel, Sa\v{s}o D\v{z}eroski}

\maketitle



\tableofcontents


%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}

This text is a user's manual for the open source machine learning system \clus{}. \clus{} is a decision tree and rule learning system that works in the \emph{predictive clustering} framework \cite{Blockeel1998icml}.
% Should we avoid one (or two) sentence paragraphs?
While most decision tree learners induce classification or regression trees, \clus{} generalizes this approach by learning trees that are interpreted as cluster hierarchies. We call such trees predictive clustering trees or PCTs. Depending on the learning task at hand, different goal criteria are to be optimized while creating the clusters, and different heuristics will be suitable to achieve this.

Classification and regression trees are special cases of PCTs, and by choosing the right parameter settings \clus{} can closely mimic the behavior of tree learners such as CART \cite{Breiman84:other} or C4.5 \cite{Quinlan1993}.  However, its applicability goes well beyond classical classification or regression tasks: \clus{} has been successfully applied to many different tasks including multi-task learning (multi-target classification and regression), structured output learning, multi-label classification, hierarchical classification, and time series prediction. Next to these supervised learning tasks, PCTs are also applicable to semi-supervised learning, subgroup discovery, and clustering.
%
In a similar way, predictive clustering rules (PCRs) generalize classification rule sets \cite{Clark91:proc} and also apply to the aforementioned learning tasks.

A full description of how \clus{} works is beyond the scope of this text. In this User's Manual, we focus on how to use \clus{}: how to prepare its inputs, how to interpret the outputs, and how to change its behavior with the available parameters. This manual is a work in progress and all comments are welcome.
%
For background information on the rationale behind the \clus{} system and its algorithms we refer to the following papers:

\begin{itemize} 

% Should we add some additional papers here?
\item H. Blockeel, L. De Raedt, and J. Ramon, Top-down induction of clustering trees, Proceedings of the 15th International Conference on Machine Learning (J. Shavlik, ed.), pp. 55-63, 1998.

\item B.~{\v{Z}}enko and S.~D{\v{z}}eroski. Learning classification rules for multiple target attributes. In Advances in Knowledge Discovery and Data Mining, pp. 454-465, 2008.

\item H. Blockeel, S. D\v zeroski, and J. Grbovi\'c, Simultaneous prediction of multiple chemical parameters of river water quality with TILDE, Proceedings of the Third European Conference on Principles of Data Mining and Knowledge Discovery (J.M. \.{Z}ytkow and J. Rauch, eds.), vol 1704, LNAI, pp. 32-40, 1999.

\item  C. Vens, J. Struyf, L. Schietgat, S. D\v zeroski and H. Blockeel, Decision trees for hierarchical multi-label classification. Machine Learning 73(2):185-214, 2008.
\end{itemize}

A longer list of publications describing different aspects and applications of \clus{} is available on the \clus{} web site
(\url{www.cs.kuleuven.be/~dtai/clus/publications.html}).

%%%%%%%%%%%%%%%%%%
\chapter{Tutorial}

\section{Installing and Running \clus}
\label{sec:run}

\begin{sloppypar}
\clus{} is written in the Java programming language, which is available from {\tt http://java.sun.com}. You will need Java version 1.5.x or newer. To run \clus{}, it suffices to install the Java Runtime Environment (JRE). If you want to make changes to \clus{} and compile its source code, then you will need to install the Java Development Kit (JDK) instead of the JRE.
\end{sloppypar}

The \clus\ software is released under the GNU General Public License version 3 or later and is available for download at \url{http://www.cs.kuleuven.be/~dtai/clus/}. After downloading \clus{}, unpack it into a directory of your choice. \clus{} is a command line application and should be started from the command prompt (Windows) or a terminal window (Unix). To start \clus{}, enter the command:
\begin{flushleft}
\verb^java -jar $CLUS_DIR/Clus.jar^ {\em filename}\verb^.s^
\end{flushleft}

\noindent{}with \verb^$CLUS_DIR/Clus.jar^ the location of \verb^Clus.jar^ in your \clus{} distribution and {\tt {\em filename}.s} the name of your settings file. In order to verify that your \clus\ installation is working properly, you might try something like:

\begin{list}{\labelitemi}{\leftmargin=1.5cm}
\item[Windows:]\mbox{}
\begin{verbatim}
cd C:\Clus\data\weather
java -jar ..\..\Clus.jar weather.s
\end{verbatim}

\item[Unix:]\mbox{}
\begin{verbatim}
cd $HOME/Clus/data/weather
java -jar ../../Clus.jar weather.s
\end{verbatim}
\end{list}

\noindent{}This runs \clus{} on a simple example \emph{Weather.} You can also try other example data sets in the \texttt{data} directory of the \clus{} distribution.

Note that the above instructions are for running the pre-compiled version of \clus{} (\texttt{Clus.jar}), which is included with the \clus{} download. If you have modified and recompiled \clus{}, or if you are using the CVS version, then you should run \clus{} in a different way, as is explained in Chapter~\ref{ch:devel}. If you want get direct CVS access, please contact the developers.

\section{Input and Output Files for \clus}

\begin{figure}%[tb]
\includegraphics{fig/clusinout}
\caption{\label{fig:iofiles}Input and output files of \clus.}
% .xval and predictions are not always produced - should we mention this in the figure?
\end{figure}

\begin{figure}%[tb]
\hrule\vspace{1em}
\begin{verbatim}
[Attributes]
Descriptive = 1-2
Target = 3-4
Clustering = 3-4

[Tree]
Heuristic = VarianceReduction
\end{verbatim}
\hrule
\caption{The settings file (\texttt{weather.s}) for the \emph{Weather} example.}
\label{weathers:fig}
\end{figure}
\begin{figure}%[tb]
\hrule\vspace{1em}
\begin{verbatim}
@RELATION "weather"

@ATTRIBUTE outlook     {sunny,rainy,overcast}
@ATTRIBUTE windy       {yes,no}
@ATTRIBUTE temperature numeric
@ATTRIBUTE humidity    numeric

@DATA
sunny,    no,  34, 50
sunny,    no,  30, 55
overcast, no,  20, 70 
overcast, yes, 11, 75
rainy,    no,  20, 88
rainy,    no,  18, 95 
rainy,    yes, 10, 95 
rainy,    yes, 8,  90
\end{verbatim}
\hrule
\caption{The training data (\texttt{weather.arff}) for the \emph{Weather} example (in Weka's ARFF format).}
\label{weatherarff:fig}
\end{figure}

\begin{figure}[tb]
\hrule\vspace{1em}
\begin{verbatim}
Clus run "weather"
******************
Date: 1/10/10 12:23 PM
File: weather.out
Attributes: 4 (input: 2, output: 2)

[Data]
File = weather.arff

[Attributes]
Target = 3-4
Clustering = 3-4
Descriptive = 1-2

[Tree]
Heuristic = VarianceReduction
PruningMethod = M5

Statistics
----------
Induction Time: 0.017 sec
Pruning Time: 0.001 sec
Model information
     Original: Nodes = 7 (Leaves: 4)
     Pruned: Nodes = 3 (Leaves: 2)

Training error
--------------
Number of examples: 8
Mean absolute error (MAE)
   Default        : [7.125,14.75]: 10.9375
   Original       : [2.125,2.75]: 2.4375
   Pruned         : [4.125,7.125]: 5.625
Mean squared error (MSE)
   Default        : [76.8594,275.4375]: 176.1484
   Original       : [6.5625,7.75]: 7.1562
   Pruned         : [19.4375,71.25]: 45.3438

Original Model
**************
outlook = sunny
+--yes: [32,52.5]: 2
+--no:  outlook = rainy
        +--yes: windy = yes
        |       +--yes: [9,92.5]: 2
        |       +--no:  [19,91.5]: 2
        +--no:  [15.5,72.5]: 2

Pruned Model
************
outlook = sunny
+--yes: [32,52.5]: 2
+--no:  [14.5,85.5]: 6
\end{verbatim}
\hrule
\caption{The \emph{Weather} example's output (\texttt{weather.out}). (Some parts have been omitted for brevity.)}
\label{weatherout:fig}
\end{figure}

\clus{} uses (at least) two input files and these are named {\tt {\em filename}.s} and {\tt {\em filename}.arff}, with {\tt {\em filename}} a name chosen by the user.  The file {\tt {\em filename}.s} contains the parameter settings for \clus{}. The file {\tt {\em filename}.arff} contains the training data to be read. The format of the data file is Weka's ARFF format\footnote{\url{http://weka.wikispaces.com/ARFF}}. The results of a \clus{} run are put in an output file {\tt {\em filename}.out}. Figure~\ref{fig:iofiles} gives an overview of the input and output files supported by \clus{}. The format of the data files is described in detail in Chapter~\ref{ch:data}, the format of the settings file is discussed in Chapter~\ref{ch:sett}, and the output files are covered in Chapter~\ref{ch:output}. Optionally, \clus\ can also generate a detailed output of the cross-validation (\texttt{weather.xval}) and model predictions in ARFF format.

In the \emph{Weather} example of Section~\ref{sec:run}, \clus{} will read its settings from the input file \texttt{weather.s} (shown in Figure~\ref{weathers:fig}) and its input data from the file \texttt{weather.arff} (Figure~\ref{weatherarff:fig}). It will then construct (with these settings) a PCT, which it will write to the output file \texttt{weather.out} (Figure~\ref{weatherout:fig}).

The \emph{Weather} example is a small multi-target or multi-task learning problem \cite{Caruana97:jrnl} in which the goal is to predict the target attributes \emph{temperature} and \emph{humidity} from the input attributes \emph{outlook} and \emph{windy.} The details about the learning task are specified in the settings file (Figure~\ref{weathers:fig}). Our example includes values for four parameters. The parameters under the heading \texttt{[Attributes]} specify how the attributes are to be used. Here, the first two attributes (attributes 1-2) are to be used in the cluster descriptions, that is, in the tests that appear in the PCT's nodes. The last two attributes (attributes 3-4) are to be predicted. The setting \texttt{Clustering = 3-4} indicates that the clustering heuristic, which is used to construct the PCT, should be computed based on the target attributes only. Finally, \texttt{Heuristic = VarianceReduction} specifies that the heuristic is variance reduction. Chapter~\ref{ch:sett} provides a detailed description of each setting supported by \clus{}.

\clus{} writes the resulting PCT together with a number of statistics such as the training set error and the test set error (if a test set has been provided) to its output file (Figure~\ref{weatherout:fig}).  In this example, the resulting tree is a multi-target tree: each leaf predicts a vector of which the first component is the predicted \emph{temperature} and the second component the predicted \emph{humidity.} Such a multi-target tree has several advantages over constructing a separate regression tree for each target variable. The most obvious one is the number of models: the user only has to interpret one tree instead of at one tree for each target. A second advantage is that the tree makes features that are relevant to all target variables explicit. For example, the first leaf of the tree in Figure~\ref{weatherout:fig} shows that \texttt{outlook = sunny} implies both a high temperature and a low humidity. Finally, due to the concept of inductive transfer, multi-target PCTs may also be more accurate than regression trees. More information about multi-target trees can be found in the following publications: \cite{Blockeel1998icml, Blockeel99:proc, Struyf06-KDID:proc, Piccart08-DS-:proc}.

% \chapter{Learning Algorithms}
% 
% \section{Tree Learning}
% 
% \subsection{Top-down Induction}
% 
% \subsection{Beam Search}
% 
% \subsection{Constrained Induction}
% 
% \subsection{Ensembles}
% 
% \section{Rule Learning}
% 
% \subsection{Sequential Covering Algorithm}
% 
% \subsection{Rule Ensembles}
% 
% \chapter{Case Studies}
%  
% \section{Multi-target Regression and Classification}
% 
% \section{Hierarchical Multi-label Classification}
% 
% \section{Time Series Clustering}
% 
% \section{Clustering with Instance Level Constraints}

\chapter{Input Format}
\label{ch:data}

Like many machine learning systems, \clus\ learns from tabular data. These data are assumed to be in the ARFF format that is also used by the Weka data mining tool.  Full details on ARFF can be found elsewhere\footnote{\url{http://weka.wikispaces.com/ARFF}}. We here only give a minimal description.

In the data table, each row represents an instance, and each column represents an attribute of the instances.  Each attribute has a name and a domain (the domain is the set of values it can take). In the ARFF format, the names and domains of the attributes are declared up front, before the data are given. The syntax is not case sensitive.
%
An ARFF file follows the following format:

\begin{tabbing}
{\tt \% all comment lines are optional, start with \%, and can occur }\\
{\tt \% anywhere in the file}\\
\\
{\tt @RELATION} name\\
\\
{\tt @ATTRIBUTE} name domain\\
{\tt @ATTRIBUTE} name domain\\
...\\
\\
{\tt @DATA}\\
value$_1$, value$_2$, ... , value$_n$\\
value$_1$, value$_2$, ... , value$_n$\\
\end{tabbing}

\noindent The domain of an attribute can be one of:
\begin{itemize}
	\item \texttt{numeric}
	\item \texttt{\{} nomvalue$_1$, nomvalue$_2$, ... , nomvalue$_n$ \texttt{\}}
	\item \texttt{string}
	\item \texttt{hierarchical} hvalue$_1$, hvalue$_2$, ... , hvalue$_n$
	\item \texttt{timeseries}
\end{itemize}
%
The first option, \texttt{numeric} (\texttt{real} and \texttt{integer} are also legal and are treated in the same way), indicates that the domain is the set of real numbers. The second type of domain is called a discrete domain.  Discrete domains are defined by enumerating the values they contain. These values are nominal. The third domain type is \texttt{string} and can be used for attributes containing arbitrary textual values.

The fourth type of domain is called \texttt{hierarchical} (multi-label). It implies two things: first, the attribute can take as a value a \emph{set of values} from the domain, rather than just a single value; second, the domain has a hierarchical structure.  The elements of the domain are typically denoted $v_1/v_2/.../v_i$ with $i \leq d$ where $d$ is the depth of the hierarchy. This type of domain is useful in the context of hierarchical multi-label classification and is not part of the standard ARFF syntax.  

The values in a	 row occur in the same order as the attributes: the $i$'th value is assigned to the $i$'th attribute.  Values in the rows must be real values if the domain of the corresponding attribute is continuous, or a member of the specified set of values if the domain is discrete.  For hierarchical multi-label attributes, a set of values from the domain can be given; this set is written by listing its elements separated by \verb^@^.

The last type of domain is \texttt{timeseries}. A time series is a fixed length series of numeric data where individual numbers are written in brackets and separated with commas. All time series of a given attribute must be of the same length. This domain type is also not part of the standard ARFF syntax.

\clus\ also supports sparse ARFF format where only non-zero data values are stored. The header of a sparse ARFF file is the same, but each data instance is written in curly braces and each attribute value is written as an attribute number (starting from zero) and the value separated by space; values of different attributes are separated by commas.

Figure~\ref{weatherarff:fig} shows an example of an ARFF file. An example of a table containing hierarchical multi-label attributes is shown in Figure~\ref{arffhmc:fig}, an example ARFF file with a time series attribute is shown in Figure~\ref{arfftimeser:fig}, and an example sparse ARFF file is shown in Figure~\ref{arffsparse:fig}.

\begin{figure}[tb]
\hrule\vspace{1em}
\begin{verbatim}
@RELATION HMCNewsGroups

@ATTRIBUTE word1   {1,0}
...
@ATTRIBUTE word100 {1,0}
@ATTRIBUTE class hierarchical rec/sport/swim,rec/sport/run,rec/auto,alt/atheism,...

@DATA
1,...,1,rec/sport/swim
1,...,1,rec/sport/run
1,...,1,rec/sport/run@rec/sport/swim
1,...,0,rec/sport
1,...,0,rec/auto
0,...,0,alt/atheism
...
\end{verbatim}
\hrule\vspace{1em}
\caption{An ARFF file that includes an hierarchical multi-label attribute.}
\label{arffhmc:fig}
\end{figure}

\begin{figure}[tb]
\hrule\vspace{1em}
\begin{verbatim}
@RELATION GeneExpressionTimeSeries

@ATTRIBUTE geneid string
@ATTRIBUTE GO0000003 {1,0}
@ATTRIBUTE GO0000004 {1,0}
...
@ATTRIBUTE GO0051704 {1,0}
@ATTRIBUTE GO0051726 {1,0}
@ATTRIBUTE target timeseries

@DATA
YAL001C,0,0,0,0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,[0.07, 0.15, 0.14, 0.15,-0.11, 0.07,-0.41]
YAL002W,0,0,0,0,0,0,0,0,0,0,1,0,0,...,1,1,0,0,[0.14, 0.14, 0.18, 0.14, 0.17, 0.13, 0.07]
YAL003W,0,0,0,0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,[0.46, 0.33, 0.04,-0.60,-0.64,-0.51,-0.36]
YAL005C,0,0,0,0,0,0,0,0,0,0,0,0,0,...,1,1,0,0,[0.86, 1.19, 1.58, 0.93, 1,    0.85, 1.24]
YAL007C,0,0,0,0,0,0,0,0,0,0,0,0,0,...,1,1,0,0,[0.12, 0.49, 0.62, 0.49, 0.84, 0.89, 1.08]
YAL008W,0,1,0,0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,[0.49, 1.01, 1.33, 1.23, 1.32, 1.03, 1.14]
...
\end{verbatim}
\hrule
\caption{An ARFF file that includes an time series attribute.}
\label{arfftimeser:fig}
\end{figure}

\begin{figure}[tb]
\hrule\vspace{1em}
\begin{verbatim}
@RELATION SparseData

@ATTRIBUTE a1    numeric
@ATTRIBUTE a2    numeric
...
@ATTRIBUTE a10   numeric
@ATTRIBUTE a11   numeric
@ATTRIBUTE class {pos,neg}

@DATA
{1 3.1, 8 2.5, 12 pos}
{7 2.3, 12 neg}
{2 8.5, 3 1.3, 12 neg}
{1 3.2, 12 pos}
{1 3.3, 8 2.7, 12 pos}
...
\end{verbatim}
\hrule
\caption{An ARFF file in sparse format.}
\label{arffsparse:fig}
\end{figure}

%
%%%%%%%%%%%%%%%%%%%%%%
\chapter{Settings File}
\label{ch:sett}

The algorithms included in the \clus{} system have a number of parameters that influence their behavior.  Most parameters have a default setting; the specification of a value for such parameters is optional.  For parameters that do not have a default setting or which should get another value than the default, a value must be specified in the settings file, {\tt {\em filename}.s}.

The settings file is structured into sections.  Each parameter belongs to a particular section.  Including the section headers (section names written in brackets) is optional, however; these headers are meant to help users structure the settings and their use is recommended.

We here explain the most common settings. Some settings that are connected to experimental or not yet fully implemented features of \clus\ are either marked as such or not presented at all.  Figure~\ref{settings:fig} shows  an example of a settings file. All the settings (including the default ones) that were used in a specific \clus\ run are printed at the beginning of the output file (\texttt{\emph{filename}.out}).

In the following, we use the convention that $n$ is an integer, $r$ is a real, $v$ is a vector of real values, $s$ is a string, $y$ is an element of \{ {\tt Yes}, {\tt No} \}, $r$ is an range of attribute indices, and $o$ is another type of value.  Strings are denoted without quotes. A vector is denoted as $[r_1,\ldots,r_n]$. An attribute range is a comma separated list of integers or intervals or \texttt{None} if the range is empty. For example, {\tt 5,7-9} indicates attributes 5, 7, 8 and 9. The first attribute in the dataset is attribute 1. Run {\tt clus -info {\em filename}.s} to list all attributes together with their indices. We now explain the settings organized into sections.

\section{General}

\begin{itemize}
\item {\tt RandomSeed = $n$}\label{sett:seed} : $n$ is used to initialize the random generator.
Some procedures used by \clus{} (e.g., creation of cross-validation folds) are randomized, and as a result, different runs of \clus{} on identical data may still yield different outputs.  When \clus{} is run on identical input data with the same {\tt RandomSeed} setting, it is guaranteed to yield the same results.
\end{itemize}

\section{Data}

\begin{itemize}
\item {\tt File = $s$} : $s$ is the name of the file that contains the training set.  The default value for $s$ is {\tt {\em filename}.arff}.  \clus{} can read compressed ({\tt .arff.zip}) or uncompressed ({\tt .arff}) data files. Path can also be included in the string.
\item {\tt TestSet = $o$} : when $o$ is {\tt None}, no test set is used; if $o$ is a number between 0 and 1, \clus{} will use a proportion $o$ of the data file as a separate test set (used for evaluating the model but not for training); if $o$ is a valid file name containing a test set in ARFF format, \clus{} will evaluate the learned model on this test set.
\item {\tt PruneSet = $o$} : defines whether and how to use a pruning set; the meaning of $o$ is identical as in the {\tt TestSet} setting.
\item {\tt XVal = $n$}\label{sett:xval} : $n$ is the number of folds to be used in a cross-validation.  To perform cross-validation, \clus{} needs to be run with the {\tt -xval} command line parameter.
\end{itemize}

\section{Attributes}

\begin{itemize}
\item {\tt Target = $r$} : sets the range of target attributes. The predictive clustering model will predict these attributes. If this setting is not specified, then it is equal to the index of the last attribute in the training dataset, i.e., the last attribute is the target by default. This setting overrides the \texttt{Disable} setting. This is convenient if one needs to build models that predict only a subset $S$ of all available target attributes $T$ (and other target attributes should not be used as descriptive attributes). Because {\tt Target} overrides {\tt Disable}, one can use the settings {\tt Disable = $T$} and {\tt Target = $S$} to achieve this. 

\item {\tt Clustering = $r$} : sets the range of clustering attributes. The predictive clustering heuristic that is used to guide the model construction is computed with regard to these atrributes. If this setting is not specified, then the clustering attributes are by default equal to the target attributes.

\item {\tt Descriptive = $r$} : sets the range of attributes that can be used in the descriptive part of the models. For a PCT, these attributes will be used to construct the tests in the internal nodes of the tree. For a set of PCRs, these attributes will appear in the rule conditions. If this setting is not specified, then the descriptive attributes are all attributes that are not target, key, or disabled.

\item {\tt Disable = $r$} : sets the range of attributes that are to be ignored by \clus. These attributes are also not read into memory.

\item {\tt Key = $r$} : sets the range of key attributes. A key attribute or a set of key attributes can be used as an example identifier. For example, if each instance represents a person, then the key attribute could store the person's name. Key attributes are not actually used by the induction algorithm, but they are written to output files, for example, to ARFF files with predictions. See \texttt{[Output]/WritePredictions} for an example.

\item {\tt Weights = $o$} : sets the relative weights of the different attributes in the clustering heuristic. To set the weights of all clustering attributes to 1.0, use {\tt Weights = 1}. To use as  weights $w_i = 1/\mathrm{Var}(a_i)$, with $\mathrm{Var}(a_i)$ the variance of attribute $a_i$ in the input data, use {\tt Weights = Normalize}.
\end{itemize}

\section{Model}

\begin{itemize}
\item {\tt MinimalWeight = $r$} : \clus{} only generates clusters with at least $r$ instances in each subset (tree leaves or rules). This is a standard setting used for pre-pruning of trees and rules.
\end{itemize}

\section{Tree}

\begin{itemize}
	\item {\tt FTest = $r$} : sets the f-test stopping criterion for regression; a node will only be split if a statistical F-test indicates a significant (at level $r$) reduction of variance inside the subsets.
	\item {\tt ConvertToRules = $o$} : $o$ is a subset of \texttt{\{No, Leaves, AllNodes\}}. \clus{} can convert a tree (or ensemble of trees) into a set of rules. The default setting is \texttt{No}, if set to \texttt{Leaves}, only tree leaves are converted to rules, if set to \texttt{AllNodes}, also the internal nodes of tree(s) are converted. This setting can be used for learning rule ensembles \cite{Aho2009}.
	\item \texttt{Heuristic = $o$} : $o$ is a subset of \{\raggedright\texttt{Default, ReducedError, Gain, GainRatio, % SSPD,
	VarianceReduction, MEstimate, Morishita, DispersionAdt, DispersionMlt, RDispersionAdt, RDispersionMlt%, GeneticDistance, SemiSupervised, VarianceReductionMissing
	}\}. Sets the heuristic function that is used for evaluating the clusters (splits) when generating trees or rules. Please note that this setting is used for trees as well as rules.
%	
	\begin{itemize}
		\item \texttt{Default}: default heuristic, if learning trees this is equal to \texttt{VarianceReduction}, if learning rules this setting is equal to \texttt{RDispersionMlt}.
		\item \texttt{ReducedError}: reduced error heuristic, can be used for trees.
		\item \texttt{Gain}: information gain heuristic, can be used for classification trees.
		\item \texttt{GainRatio}: information gain ratio heuristic \cite{Quinlan1986}, can be used for classification trees.
		%\item \texttt{SSPD}: sum of squared distances, can be used for regression trees,
		\item \texttt{VarianceReduction}: variance reduction heuristic, can be used for trees.
		\item \texttt{MEstimate}: $m$-estimate heuristic \cite{CestnikBratko1991}, can be used for classification trees.
		\item \texttt{Morishita}: Morishita heuristic \cite{Sese2004}, can be used for trees.% Is this correct?
		\item \texttt{DispersionAdt}: additive dispersion heuristic \cite{Zenko07} pages 37--38, can be used for rules.
		\item \texttt{DispersionMlt}: multiplicative dispersion heuristic \cite{Zenko07} pages 37--38, can be used for rules.
		\item \texttt{RDispersionAdt}: additive relative dispersion heuristic \cite{Zenko07} pages 37--38, can be used for rules.
		\item \texttt{RDispersionMlt}: multiplicative relative dispersion heuristic \cite{Zenko07} pages 37--38, can be used for rules, the default heuristic for learning predictive clustering rules.
		%\item \texttt{GeneticDistance}:
		%\item \texttt{SemiSupervised}:
		%\item \texttt{VarianceReductionMissing}:
	\end{itemize}
	\item \texttt{PruningMethod = $o$} : $o$ is a subset of \{\raggedright\texttt{Default, None, C4.5, M5, M5Multi, ReducedErrorVSB, Garofalakis, GarofalakisVSB, CartVSB, CartMaxSize}\}. Sets the post-pruning method for trees.
	\begin{itemize}
		\item \texttt{Default}: default pruning method for trees, if learning classification trees this is equal to \texttt{C4.5}, if learning regression trees this is equal to \texttt{M5}. % Is this true?
		\item \texttt{None}: no post-pruning of learned trees is performed.
		\item \texttt{C4.5}: pruning as in C4.5 \cite{Quinlan1993}, can be used for classification trees,
		\item \texttt{M5}: pruning as in M5 \cite{Quinlan1992},	can be used for regression trees,
		\item \texttt{M5Multi}: M5 \cite{Quinlan1992} pruning generalized for multi-target regression trees	% Is this true?		
		\item \texttt{ReducedErrorVSB}: 
		\item \texttt{Garofalakis}: pruning method proposed by Garofalakis et al. \cite{Garofalakis03:jrnl} used for constraint induction of trees. % Is this true?
		\item \texttt{GarofalakisVSB}: 
		\item \texttt{CartVSB}: pruning as in the CART book \cite{cart84}.
		\item \texttt{CartMaxSize}: 
	\end{itemize}
\end{itemize}

\section{Rules}

Please note that the parameter for selecting rule learning heuristic in actually located in the \texttt{[Tree]} section!

\begin{itemize}
	\item \texttt{CoveringMethod = $o$} : $o$ is a subset of \{\texttt{Standard, % WeightedMultiplicative, WeightedAdditive, 
	WeightedError, %Union, BeamRuleDefSet, 
	RandomRuleSet, %StandardBootstrap, 
	HeurOnly, RulesFromTree}\}, defines how the rules are generated.
	\begin{itemize}
		\item \texttt{Standard}: standard covering algorithm \cite{Michalski1969}, all examples covered by the new rule are removed from the current learning set, can be used for learning ordered rules.
		%\item \texttt{WeightedMultiplicative}: 
		%\item \texttt{WeightedAdditive}: 
		\item \texttt{WeightedError}: error weighted covering algorithm \cite{Zenko07} (Section 4.5), examples covered by the new rule are not removed from the current learning set, but their weight is decreased inversely proportional to the error the new rule makes when predicting their target values, can be used for learning unordered rules.
		%\item \texttt{Union}: 
		%\item \texttt{BeamRuleDefSet}: 
		\item \texttt{RandomRuleSet}: rules are generated randomly, (experimental feature).
		%\item \texttt{StandardBootstrap}: 
		\item \texttt{HeurOnly}: no covering is used, the heuristic function takes into account the already learned rules and the examples they cover to focus on yet uncovered examples, (experimental feature).
		\item \texttt{RulesFromTree}: rules are not learned with the covering approach, but a tree is learned first and then transcribed into a rule set.
	\end{itemize}
	\item \texttt{CoveringWeight = $r$} : weight controlling the amount by which weights of covered examples are reduced within the error weighted covering algorithm -- $\zeta$ in \cite{Zenko07} (Section 4.5, Equations 4.6 and 4.8), valid values are between 0 and 1, by default it is set to 0.1, can be used for unordered rules with error weighted covering method.
	\item \texttt{RuleAddingMethod = $o$} : $o$ is a subset of \{\texttt{Always, IfBetter, IfBetterBeam}\}, defines how rules are added to the rule set.
	\begin{itemize}
		\item \texttt{Always}: each rule when constructed is always added to the rule set,
		\item \texttt{IfBetter}: rule is only added to the rule set if the performance of the rule set with the new rule is better than without it,
		\item \texttt{IfBetterBeam}: similar to \texttt{IfBetter}, but if the rule does not improve the performance of the rule set, other rules from the beam are also evaluated and possibly added to the rule set.
	\end{itemize}
	The default value is \texttt{Always}, for regression rules setting this option to \texttt{IfBetter} is recommended.
	\item \texttt{PrintRuleWiseErrors = $y$}: If \texttt{Yes},  \clus{} will print error estimation for each rule separately.
%	\item \texttt{PredictionMethod = $o$} : $o$ is a subset of \{\texttt{DecisionList", "TotCoverageWeighted", %"CoverageWeighted", "AccuracyWeighted",
%		"AccCovWeighted", "EquallyWeighted", "Optimized", "Union", "GDOptimized", "GDOptimizedBinary"
	\item {\tt ComputeDispersion = $y$}: If \texttt{Yes},  \clus{} will print some additional dispersion estimation for each rule and entire rule set.
	\item \texttt{OptGDMaxIter = $n$}: $n$ defines a number of iterations that a gradient descent algorithm for
optimizing rule weights makes, used for learning rule ensembles \cite{Aho2009}. The default value is 1000.
	\item \texttt{OptGDMaxNbWeights = $n$}: $n$ defines a maximum number of of allowed nonzero weights for
rules/linear terms, used for learning rule ensembles \cite{Aho2009}. If we have enough modified weights, only the nonzero ones are altered for the rest of the optimization. With this we can limit the size of the rule set. The default value of 0 means no rule set size limitation.
	\item \texttt{OptGDGradTreshold = $r$} : the $\tau$ treshold value for the gradient descent (GD) algorithm used for learning rule ensembles \cite{Aho2009}. $\tau$ defines the limit by which gradients are changed during every iteration of the GD algorithm. If $\tau$=1 effect is similar to L1 regularization (Lasso) and $\tau$=0 the effect is similar to L2. If \texttt{OptGDMaxNbWeights} is used, setting $\tau$=1 is usually enough (it is the fastest). Possible values are from the [0,1] interval, the default is 1.
	\item \texttt{OptGDNbOfTParameterTry = $n$}: $n$ defines how many different $\tau$ values are checked between \texttt{Opt\-GD\-Grad\-Treshold} and 1. We use a validation set to compute, which $\tau$ value gives the best accuracy. If \texttt{OptGDMaxNbWeights} is used, usually only a single value $\tau$=1 is enough (fastest).
\end{itemize}


\section{Ensembles}
\begin{itemize}
	\item \texttt{Iterations = $n$}: $n$ defines a number of base-level models (trees) in the ensemble, by default it is set to 10.
	\item \texttt{EnsembleMethod = $o$} : $o$ is a subset of \{\texttt{Bagging, RForest, RSubspaces, BagSubspaces}\} defines the ensemble method.
	\begin{itemize}
		\item \texttt{Bagging}: Bagging \cite{Breiman}
		\item \texttt{RForest}: Random forest \cite{Breiman}
		\item \texttt{RSubspaces}: Random Subspaces \cite{Tin Kam Ho}
		\item \texttt{BagSubspaces}: Bagging of subspaces \cite{Panov and Dzeroski, IDA 2007}
	\end{itemize}
	\item \texttt{VotingType = $o$} : $o$ is a subset of \{\texttt{Majority, ProbabilityDistribution}\} selects the voting scheme for combining predictions of base-level models.
	\begin{itemize}
		\item \texttt{Majority}: each base-level model casts one vote, for regression this is equal to averageing
		\item \texttt{ProbabilityDistribution}: each base-level model casts probability distributions for each target attribute, does not work for regression
	\end{itemize}
	Kohavi et al \cite{Kohavi} recommend the latter option.
	\item \texttt{SelectRandomSubspaces = $n$}: $n$ defines size of feature subset for random forests, random subspaces and bagging of subspaces. Default setting is 0, which means floor(log with basis 2 of the number of descriptive attributes +1) as recommended by Breiman \cite{, Random Forest, Machine Learning Journal}
	\item {\tt PrintAllModels = $y$}: If \texttt{Yes}, \clus\ will print all base-level models of an ensemble in the output file. The default setting is \texttt{No}.
	\item {\tt PrintAllModelFiles = $y$}: If \texttt{Yes}, \clus\ will save all base-level models of an ensemble in the model file. The default setting is \texttt{No}, which prevents from creating very large model files.
	\item {\tt Optimize = $y$}: If \texttt{Yes}, \clus\ will optimize memory usage during learning. The default setting is \texttt{No}.
	\item {\tt OOBestimate = $y$}: If \texttt{Yes}, out-of-bag estimate of the performance of the ensemble will be done. The default setting is \texttt{No}.
	\item {\tt FeatureRanking = $y$}: If \texttt{Yes}, feature ranking via random forests will be performed. The default setting is \texttt{No}.
	\item {\tt EnsembleRandomDepth = $y$}: If \texttt{Yes}, different random depth for each base-level model is selected. Used, e.g., in rule ensembles. The \texttt{MaxDepth} setting from \texttt{[Tree]} section is used as average. The default setting is \texttt{No}.
%BagSelection = -1
	% 'Quasi' parallel implementation for ensembles
	% its value is an integer ranging from -1 to the number of trees in the ensemble
	% if set to -1, then the parallel implementation is discarded
	% if set to 0, then CLUS combines the predictions from the different bags
	% if set to i, i in 1..EnsembleSize, then CLUS learns the model for the i-th bag
	% if set as interval [val1-val2], this means to learn all bags between val1 and val2, including val1 and val2
	
	 
\end{itemize}

\section{Constraints}

\begin{itemize}
\item {\tt Syntactic = $o$} : sets the file with syntactic constraints (e.g., a partial tree) \cite{Struyf06-KDID:proc}
\item {\tt MaxSize = $o$} : sets the maximum size for Garofalakis pruning \cite{Garofalakis03:jrnl, Struyf06-KDID:proc}; $o$ can be a positive integer or {\tt Infinity}
\item {\tt MaxError = $o$} : sets the maximum error for Garofalakis pruning; $o$ is a positive real or {\tt Infinity}
\item {\tt MaxDepth = $o$} : $o$ is a positive integer or {\tt Infinity}. \clus{} will build trees with depth at most $o$. In the context of rule ensemble learning \cite{Aho2009}, this sets the average maximum depth of trees that are then converted into rules and a value of 3 seems to work fine.
\end{itemize}

\section{Output}

\begin{itemize}
\item {\tt AllFoldModels = $y$} : if set to {\tt Yes}, \clus{} will output the model built in each fold of a cross-validation
\item {\tt AllFoldErrors = $y$} : if set to {\tt Yes}, \clus{} will output the test set error (and other evaluation measures) for each fold
\item {\tt TrainErrors = $y$} : if set to {\tt Yes}, \clus{} will output the training set error (and other evaluation measures)
\item {\tt UnknownFrequency = $y$} : if set to {\tt Yes}, \clus{} will show in each node of the tree the proportion of instances that had a missing value for the test in that node
\item {\tt BranchFrequency = $y$} : if set to {\tt Yes}, \clus{} will show in each node of the tree, for each possible outcome of the test in that node, the proportion of instances that had that outcome
\item {\tt WritePredictions = $o$} : $o$ is a subset of \{Train,Test\}. If $o$ includes ``Train'', then the prediction for each training instance will be written to an ARFF output file. The file is named {\tt {\em filename}.train.{\em i}.pred.arff} with $i$ the iteration. In a single run, $i = 1$. In a 10 fold cross-validation, $i$ will vary from 1 to 10. If $o$ includes ``Test'', then the predictions for each test instance will be written to disk. The file is named {\tt {\em filename}.test.pred.arff}.
\end{itemize}

\section{Beam}

\begin{itemize}
\item {\tt SizePenalty = $o$} : sets the size penalty parameter used in the beam heuristic \cite{Kocev07a:proc}
\item {\tt BeamWidth = $n$} : sets the width of the beam used in the beam search performed by \clus{} \cite{Kocev07a:proc}
\item {\tt MaxSize = $o$} : sets the maximum size constraint \cite{Kocev07a:proc}; $o$ is a positive integer or {\tt Infinity}
\end{itemize}

\begin{figure}[tb]
\hrule\vspace{1em}
\begin{verbatim}
[General]
RandomSeed = 0              % seed of random generator

[Data]
File = weather.arff         % training data
TestSet = None              % data used for evaluation (file name / proportion)
PruneSet = None             % data used for tree pruning (file name / proportion)
XVal = 10                   % number of folds in cross-validation (clus -xval ...)

[Attributes]
Target = 5                  % index of target attributes
Disable = 4                 % Disables some attributes (e.g., "5,7-8")
Key = None                  % Sets the index of the key attribute
Weights = Normalize         % Normalize numeric attributes

[Model]
MinimalWeight = 2.0         % at least 2 examples in each subtree
         
[Tree]
FTest = 1.0                 % f-test stopping criterion for regression
ConvertToRules = No         % Convert the tree to a set of rules

[Constraints]
Syntactic = None            % file with syntactic constraints (a partial tree)
MaxSize = Infinity          % maximum size for Garofalakis pruning
MaxError = Infinity         % maximum error for Garofalakis pruning
MaxDepth = Infinity         % Stop building the tree at the given depth

[Output]
AllFoldModels = Yes         % Output model in each cross-validation fold
AllFoldErrors = No          % Output error measures for each fold
TrainErrors = Yes           % Output training error measures
UnknownFrequency = No       % proportion of missing values for each test
BranchFrequency = No        % proportion of instances for which test succeeds
WritePredictions = {Train,Test}    % write test set predictions to file

[Beam]
SizePenalty = 0.1           % size penalty parameter used in the beam heuristic
BeamWidth = 10              % beam width
MaxSize = Infinity          % Sets the maximum size constraint
\end{verbatim}
\hrule
\caption{An example settings file}
\label{settings:fig}
\end{figure}

%
\section{Hierarchical}

A number of settings are relevant only when using \clus{} for Hierarchical Multi-label Classification (HMC).  These go in the separate section ``Hierarchical''.  The most important ones are:

\begin{itemize}
\item {\tt Type = $o$} : $o$ is {\tt Tree} or {\tt DAG}, and indicates whether the class hierarchy is a tree or a directed acyclic graph \cite{Vens08:jrnl}
\item {\tt WType = $o$} : defines how parents' class weights are aggregated in DAG-shaped hierarchies (\cite{Vens08:jrnl}, Section 4.1): possible values are {\tt ExpSumParentWeight}, {\tt ExpAvgParentWeight}, {\tt ExpMinParentWeight}, {\tt ExpMaxParentWeight}, and {\tt NoWeight}.  These define the weight of a class to be $w_0$ times the sum, average, minimum or maximum of the parent's weights, respectively, or to be 1.0 for all classes. 
\item {\tt WParam = $r$} : sets the parameter $w_0$ used in the formula for defining the class weights (\cite{Vens08:jrnl}, Section 4.1)
\item {\tt HSeparator = $o$} : $o$ is the separator used in the notation of values of the hierarchical domain (typically `/' or `.') 
\item {\tt EmptySetIndicator = $o$} : $o$ is the symbol used to indicate the empty set
\item {\tt OptimizeErrorMeasure = $o$} : \clus{} can automatically optimize the {\tt FTest} setting; $o$ indicates what criterion should be maximized for this (\cite{Vens08:jrnl}, Section 5.2).  Possible values for $o$ are:
  \begin{itemize}
   \item {\tt AverageAUROC}: average of the areas under the class-wise ROC curves
   \item {\tt AverageAUPRC}: average of the areas under the class-wise precision-recall curves
   \item {\tt WeightedAverageAUPRC}: similar to AverageAUPRC, but each class's contribution is weighted by its relative frequency
   \item {\tt PooledAUPRC}: area under the average (or pooled) precision-recall curve
  \end{itemize}
\item {\tt ClassificationThreshold = $o$} : The original tree constructed by \clus{} contains a vector of predicted probabilities (one for each class) in each leaf. Such a probabilistic prediction can be converted into a set of labels by applying a threshold $t$: all labels that are predicted with probability $\geq t$ are in the predicted set.  $o$ can be a list of thresholds, e.g., [0.5, 0.75, 0.80, 0.90, 0.95]. \clus{} will output for each value in the set a tree in which the predicted label sets are constructed with this particular threshold. So, in the example, the output file will contain 5 trees corresponding to the thresholds 0.5, 0.75, 0.80, 0.90 and 0.95.

\item {\tt RecallValues = $o$} : $o$ is a list of recall values, e.g., [0.1, 0.2, 0.3]. For each value, \clus{} will output the average of the precisions over all class-wise precision-recall curves that correspond to the particular recall value in the output file.
\item {\tt EvalClasses = $o$} : If $o$ is {\tt None}, \clus{} computes average error measures across all classes in the class  hierarchy. If $o$ is a list of classes, then the error measures are only computed with regard to the classes in this list.
\end{itemize}

Figure~\ref{settings-hmc:fig} summarizes these briefly.

\begin{figure}[tb]
\hrule\vspace{1em}
\begin{verbatim}
[Hierarchical]
Type = Tree                         % Tree or DAG hierarchy?
WType = ExpAvgParentWeight          % aggregation of class weights
WParam = 0.75                       % parameter w_0
HSeparator = /                      % separator used in class names
EmptySetIndicator = n               % symbol for empty set
OptimizeErrorMeasure = PooledAUPRC  % FTest optimization strategy
ClassificationThreshold = None      % threshold for "positive"
RecallValues = None                 % where to report precision
EvalClasses = None                  % classes to evaluate
\end{verbatim}
\hrule
\caption{Settings specific for hierarchical multi-label classification}
\label{settings-hmc:fig}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Command Line Parameters}
\label{param:ch}

\clus{} is run from the command line.  It takes a number of command line parameters that affect its behavior.

\begin{itemize}
\item {\tt -xval} : in addition to learning a single model from the whole input dataset, perform a cross-validation.  The XVal setting (page~\pageref{sett:xval}) determines the number of folds; the RandomSeed setting (page~\pageref{sett:seed}) initializes the random generator that determines the folds.

\item {\tt -fold} $N$: run only fold $N$ of the cross-validation.

\item {\tt -rules} : construct predictive clustering rules (PCRs) instead of predictive clustering trees (PCTs).

\item {\tt -forest} : construct an ensemble instead of a single tree \cite{Kocev07b:proc}.

\item {\tt -beam} : construct a tree using beam search \cite{Kocev07a:proc}.

\item {\tt -sit} : run Empirical Asymmetric Selective Transfer \cite{Piccart08-DS-:proc}.

\item {\tt -silent} : run \clus{} with reduced screen output.

\item {\tt -info} : gives information and summary statistics about the dataset.
\end{itemize}

%	public final static String[] OPTION_ARGS = { "exhaustive", "xval", "oxval",
%			"target", "disable", "silent", "lwise", "c45", "info", "sample",
%			"debug", "tuneftest", "load", "soxval", "bag", "obag", "show",
%			"knn", "knnTree", "beam", "gui", "fillin", "rules", "weka",
%			"corrmatrix", "tunesize", "out2model", "test", "normalize",
%			"tseries", "writetargets", "fold", "forest", "copying", "sit", "tc" };

\chapter{Output Files}
\label{ch:output}

When \clus{} is finished, it writes the results of the run into an output file with the name
{\tt {\em filename}.out}.  An example of such an output file is shown in Figures \ref{output1:fig} to \ref{output4:fig}.

\begin{figure}[tb]
\hrule\vspace{1em}
\begin{verbatim}
Clus run "weather"
******************

Date: 1/10/10 4:37 PM
File: weather.out
Attributes: 4 (input: 2, output: 2)
Missing values: No

[General]
Verbose = 1
Compatibility = Latest
RandomSeed = 0
ResourceInfoLoaded = No

[Data]
File = weather.arff
TestSet = None
PruneSet = None
XVal = 10
RemoveMissingTarget = No
NormalizeData = None

[Attributes]
Target = 3-4
Clustering = 3-4
Descriptive = 1-2
Key = None
Disable = None
Weights = Normalize
ClusteringWeights = 1.0
ReduceMemoryNominalAttrs = No

[Constraints]
Syntactic = None
MaxSize = Infinity
MaxError = 0.0
MaxDepth = Infinity

[Output]
ShowModels = {Default, Pruned, Others}
TrainErrors = Yes
ValidErrors = Yes
TestErrors = Yes
AllFoldModels = Yes
AllFoldErrors = No
AllFoldDatasets = No
UnknownFrequency = No
BranchFrequency = No
ShowInfo = {Count}
PrintModelAndExamples = No
WriteErrorFile = No
WritePredictions = {None}
WriteModelIDFiles = No
WriteCurves = No
OutputPythonModel = No
OutputDatabaseQueries = No
\end{verbatim}
\hrule
\caption{Example output file (part 1, settings).}
\label{output1:fig}
\end{figure}

\begin{figure}[tb]
\hrule\vspace{1em}
\small
\begin{verbatim}
[Nominal]
MEstimate = 1.0

[Model]
MinimalWeight = 2.0
MinimalNumberExamples = 0
MinimalKnownWeight = 0.0
ParamTuneNumberFolds = 10
ClassWeights = 0.0
NominalSubsetTests = Yes

[Tree]
Heuristic = VarianceReduction
PruningMethod = M5
M5PruningMult = 2.0
FTest = 1.0
BinarySplit = Yes
ConvertToRules = No
AlternativeSplits = No
Optimize = {}
MSENominal = No
\end{verbatim}
\hrule\vspace{1em}
\caption{Example output file (part 2, settings (ctd.)).}
\label{output2:fig}
\end{figure}

\begin{figure}[tb]
\hrule\vspace{1em}
\footnotesize
\begin{verbatim}
Run: 01
*******

Statistics
----------

FTValue (FTest): 1.0
Induction Time: 0.018 sec
Pruning Time: 0.001 sec
Model information
     Default: Nodes = 1 (Leaves: 1)
     Original: Nodes = 7 (Leaves: 4)
     Pruned: Nodes = 3 (Leaves: 2)

Training error
--------------

Number of examples: 8
Mean absolute error (MAE)
   Default        : [7.125,14.75]: 10.9375
   Original       : [2.125,2.75]: 2.4375
   Pruned         : [4.125,7.125]: 5.625
Mean squared error (MSE)
   Default        : [76.8594,275.4375]: 176.1484
   Original       : [6.5625,7.75]: 7.1562
   Pruned         : [19.4375,71.25]: 45.3438
Root mean squared error (RMSE)
   Default        : [8.7669,16.5963]: 13.2721
   Original       : [2.5617,2.7839]: 2.6751
   Pruned         : [4.4088,8.441]: 6.7338
Weighted root mean squared error (RMSE) (Weights [0.013,0.004])
   Default        : [1,1]: 1
   Original       : [0.2922,0.1677]: 0.2382
   Pruned         : [0.5029,0.5086]: 0.5058
Pearson correlation coefficient
   Default        : [?,?], Avg r^2: ?
   Original       : [0.9564,0.9858], Avg r^2: 0.9432
   Pruned         : [0.8644,0.861], Avg r^2: 0.7442
\end{verbatim}
\hrule
\caption{Example output file (part 3, statistics).}
\label{output3:fig}
\end{figure}

\begin{figure}[tb]
\hrule\vspace{1em}
\begin{verbatim}
Default Model
*************

[18.875,77.25]: 8

Original Model
**************

outlook = sunny
+--yes: [32,52.5]: 2
+--no:  outlook = rainy
        +--yes: windy = yes
        |       +--yes: [9,92.5]: 2
        |       +--no:  [19,91.5]: 2
        +--no:  [15.5,72.5]: 2

Pruned Model
************

outlook = sunny
+--yes: [32,52.5]: 2
+--no:  [14.5,85.5]: 6
\end{verbatim}
\hrule
\caption{Example output file (part 4, learned models).}
\label{output4:fig}
\end{figure}

\section{Used Settings}

The first part of {\tt {\em filename}.out} (shown in Figures \ref{output1:fig} and \ref{output2:fig}) contains the values of the settings that were used for this run of \clus{}, in the format used by the settings file.  This part can be copied and pasted to {\tt {\em filename}.s} and modified for subsequent runs.

\section{Evaluation Statistics}

The next part contains statistics about the results of this \clus{} run.

Summary statistics about the running time of \clus{} and about the size of the resulting models are given.  Next, information on the models' predictive performance on the training set (``training set error") is given, as well as an estimate of its predictive performance on unseen examples (``test set error"), when available (this is the case if a cross-validation or an evaluation on a separate test set was performed).  

Typically three models are reported: a ``default'' model consisting of a tree of size zero, which can be used as a reference point (for instance, its predictive accuracy equals that obtained by always predicting the majority class); an unpruned (``original") tree, and a pruned tree.

For classification trees the information given for each model by default includes a contingency table, and (computed from that) the accuracy and Cramer's correlation coefficient.

For regression trees, this information includes the mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), weighted RMSE, the Pearson correlation coefficient $r$ and it square.  In the weighted RMSE, the weight of a given attribute $A$ is its normalization weight, which is $\frac{1}{\sqrt{\mathrm{Var}(A)}}$, with $\mathrm{Var}(A)$ equal to $A$'s variance in the input data. 

\section{The Models}

The output file contains the learned models, represented as decision trees.  The level of detail in which the models are shown is influenced by certain settings.

\chapter{Developer Documentation}
\label{ch:devel}

\section{Compiling Clus}

Note: The \clus{} download comes with a pre-compiled version of \clus{} stored in the file Clus.jar. So, if you just want to run \clus{} as it is on a data set, then you do not need to compile \clus{}. You can run it by following the instructions in Section~\ref{sec:run}. On the other hand, if you wish to modify the source code of \clus{}, or if you are using the CVS version, then you will need to compile the source code of \clus{}. This can be done using the commands
below or using the Eclipse IDE as pointed out in the next section.

\vspace*{1em}\noindent(Windows)

\begin{small}
\begin{verbatim}
cd C:\Clus\src
javac -d "bin" -cp ".;jars\commons-math-1.0.jar;jars\jgap.jar" clus/Clus.java
\end{verbatim}
\end{small}

\noindent(Unix)

\begin{small}
\begin{verbatim}
cd /home/john/Clus
javac -d "bin" -cp ".:jars/commons-math-1.0.jar:jars/jgap.jar" clus/Clus.java
\end{verbatim}
\end{small}
This will compile \clus{} and write the resulting .class files (Java executable 
byte code) to the "bin" subdirectory.
%
Alternatively, use the "./compile.sh" script provided in the \clus{} main directory.

\section{Compiling Clus with Eclipse}

In Eclipse, create a new project for \clus{} as follows:

\begin{itemize}
\item Choose \verb^File | New | Project^.
\item Select "Java Project" in the dialog box.
\item In the "New Java Project" dialog box:
 \begin{itemize}
 \item Enter "Clus" in the field "Project Name".
 \item Choose "Create project from existing source" and browse to the location where 
      you unzipped Clus. E.g., \verb^/home/john/Clus^ or \verb^C:\Clus^.
 \item Click "Next".
 \item Select the "Source" tab of the build settings dialog box.
     Change "Default output folder" (where the class files are generated) to: "Clus/bin".
 \item Select the "Libraries" tab of the build settings dialog box.
     Click "Add external jars" and add in this way these three jars:\\
        Clus/jars/commons-math-1.0.jar\\
        Clus/jars/jgap.jar\\
        Clus/jars/weka.jar
 \item Click "Finish".
 \end{itemize}
\item Select the "Navigator" view (Choose Window | Show View | Navigator)
 \begin{itemize}
   \item  Right click the "Clus" project in this view.
   \item  Select "Properties" from the context menu.
   \item  Select the "Java Compiler" tab.
   \item  Set the "Java Compliance Level" to 5.0.
 \end{itemize}
\end{itemize}
Now \clus{} should be automatically compiled by Eclipse.
To run \clus{} from Eclipse:
 \begin{itemize}
   \item Set as main class "clus.Clus".
   \item Set as arguments the name of your settings file (appfile.s).
   \item Set as working directory, the directory on the file system where your data set is located.
\end{itemize}

\section{Running Clus after Compiling the Source Code}

These instructions are for running \clus{} after you compiled its source code (using the instructions "Compiling Clus" or "Compiling Clus with Eclipse"). To run the pre-compiled version that is available in the file "Clus.jar", see Section~\ref{sec:run}.

\vspace*{1em}\noindent(Windows)
\begin{small}
\begin{verbatim}
cd path\to\appfile.s
java -cp "C:\Clus\bin;C:\Clus\jars\commons-math-1.0.jar;C:\Clus\jars\jgap.jar" 
  clus.Clus appfile.s
\end{verbatim}
\end{small}

\noindent(Unix)

\begin{small}
\begin{verbatim}
cd path/to/appfile.s
java -cp "$HOME/Clus/bin:$HOME/Clus/jars/commons-math-1.0.jar:$HOME/Clus/jars/jgap.jar" 
  clus.Clus appfile.s
\end{verbatim}
\end{small}
%
Alternatively, use the "./clus.sh" script provided in the \clus{} main directory after adjusting the line that defines CLUS\_DIR at the top of the script.

\section{Code Organization}
Here we only provide a rough guide to the \clus\ code by listing some of the key classes and packages.
\begin{description}
	\item[\texttt{clus/Clus.java}] the main class with the main method, which is called when starting \clus.
	\item[\texttt{clus/algo}] package with learning algorithms, e.g., sub-package \texttt{clus/algo/tdidt} includes tree learning algorithm and \texttt{clus/algo/rules} includes rule learning algorithm, \texttt{clus/algo/split} includes classes used for generating conditions in both trees and rules.
	\item[\texttt{clus/data}] package with sub-packages and classes related to reading and storing of the data.
	\item[\texttt{clus/error}] package where different error estimation measures are defined.
	\item[\texttt{clus/ext}] some extensions of base tree learning methods can be found here, e.g., sub-package \texttt{hierarchical} contains extensions needed for hierarchical classification, \texttt{ensembles} contains ensembles of trees and \texttt{timeseries} contains extensions for predicting time-series data.
	\item[\texttt{clus/heuristic}] contains classes implementing heuristic functions for tree learning (heuristics for rule learning are located in package \texttt{clus/algo/rules}).
	\item[\texttt{clus/main}] contains some important support classes such as:
	\begin{description}
		\item[\texttt{ClusRun.java}]
		\item[\texttt{ClusStat.java}]
		\item[\texttt{ClusStatManager.java}]
		\item[\texttt{Settings.java}] all the \clus\ settings are defined here.
	\end{description}
	\item[\texttt{clus/model}] classes used for representations of models can be found here, including tests that appear in trees and rules (\texttt{clus/model/test}).
	\item[\texttt{clus/pruning}] contains methods for tree pruning.
	\item[\texttt{clus/statistic}] contains classes used for storing and manipulating different information and statistics on data. The key classes are:
	\begin{description}
		\item[\texttt{ClusStatistic.java}] super class for all statistics used in \clus.
		\item[\texttt{ClassificationStat.java}] class for storing information on nominal attributes (e.g., counts for each possible nominal value)
		\item[\texttt{RegressionStat.java}] class for storing information on numeric attributes (e.g., sums of values and sums of squared values).
		\item[\texttt{CombStat.java}] class for storing information on nominal and numeric attributes (contains \texttt{ClassificationStat} and \texttt{RegressionStat} classes).
	\end{description}
 	\item[\texttt{clus/tools}] contains some support code, e.g., sub-package \texttt{optimization} contains optimization procedures used in rule ensemble learning.
 	\item[\texttt{clus/weka}] contains classes for interfacing with Weka machine learning package.
\end{description}

%\chapter{Examples}
%In this chapter we demonstrate some simple examples (in addition to the example on learning predictive clustering trees for multi-target prediction presented throughout the manual) how \clus\ can be used for solving different tasks. All the used data sets can be found in the directory \texttt{data/} of the \clus\ distribution. 
%\comment{Not sure yet about this ...}
%\section{Learning Rules for Multi-target Prediction}
%\section{Learning Trees for Hierarchical Multi-label Classification}
%\section{Learning Tree Ensembles}
%\section{Learning Rule Ensembles}

\appendix
\chapter{Constructing Phylogenetic Trees Using Clus}

In this appendix, we describe the use of \clusphy{}, a method for phylogenetic tree reconstruction \cite{Vens10:proc}.
Example files can be found in the following directory:  
\begin{flushleft}
\verb^$CLUS_DIR/data/phylo/.^
\end{flushleft}

The input to \clusphy{} consists of a multiple alignment (in ARFF format), a settings file, and optionally a distance matrix.

\section{Input Format}
Each position in the multiple alignment becomes an attribute in the ARFF file. The domain of these attributes is discrete and will typically consist of the values $A$, $C$, $T$, and $G$ for DNA sequences, and the amino acids for protein sequences. If gaps occur, they have to be listed in the attribute domain. Gaps are represented as `-'.
Moreover, \clusphy{} requires a string attribute, that contains an identifier for the sequence.

Perl scripts for converting PHY and FASTA files into the ARFF file format are available in the {\tt data/phylo/} directory.

\section{Settings File}
In order to construct phylogenetic trees, the settings file looks like the one shown in Fig.~\ref{phylosettingsreq:fig}. Apart from the settings shown in the figure, there are a number of extra settings, specific for phylogenetic tree reconstruction (see Fig.~\ref{phylosettingsopt:fig}). We discuss them in detail.

\begin{figure}[tb]
\hrule\vspace{1em}
\begin{verbatim}
[Tree]
PruningMethod = None    
FTest = 1.0    
AlternativeSplits = true    % Gives a listing of all equivalent mutations in the nodes 
Heuristic = GeneticDistance

[Attributes]
Key = 1                     % The identifier attribute
Descriptive = 2-897         % All attributes corresponding to the alignment
Target = 2-897              % All attributes corresponding to the alignment
Weights = 1

[Model]
MinimalWeight = 1
    
[Data]
File = chimp.arff

[Output]
TrainErrors = No
PrintModelAndExamples = true
\end{verbatim}
\hrule
\caption{Required settings for learning phylogenetic trees}
\label{phylosettingsreq:fig}
\end{figure}

\begin{figure}[tb]
\hrule\vspace{1em}
\begin{verbatim}
[Phylogeny]
Sequence = DNA
OptimizationCriterion = MinBranchLengths
DistanceMatrix = mydistancematrix
DistanceMeasure = JC
\end{verbatim}
\hrule
\caption{Optional settings for learning phylogenetic trees}
\label{phylosettingsopt:fig}
\end{figure}

\begin{itemize}
\item {\tt Sequence = $o$} : defines which type of sequences is used. Possible values for $o$ are {\tt DNA}, {\tt Protein}, or {\tt Any}. The latter can be used in case a different alphabet than nucleotides or amino acids is used. The default value for $o$ is {\tt DNA}.
\item {\tt OptimizationCriterion = $o$} : defines which criterion is optimized in the phylogentic tree heuristic. Possible values for $o$ are {\tt MinTotBranchLength}, for minimizing the total branch length of the tree, and {\tt MaxAvgPWDistance}, for maximizing the average pairwise distance between two subclusters (i.e., as done in the PTDC algorithm \cite{Arslan07:proc}). The default value for $o$ is {\tt MinTotBranchLength}.
\item {\tt DistanceMatrix = $s$} : $s$ is the name of the optional pairwise distance matrix file. This file has to be formatted in the same way as in the Phylip software package \cite{Phylip3.6-08:misc}, i.e., the first line contains the number of sequences, and then for each sequence there is a line starting with its identifier and listing the pairwise distances to all the other sequences. See the example in the file \emph{mydistancematrix}. The rows in the distance matrix need to have the same ordering as the rows in the ARFF file.
\item {\tt DistanceMeasure = $o$} : defines which genetic distance is used between pairs of sequences, in the case no distance matrix is given. Possible values for $o$ are {\tt Edit} for edit distance, {\tt PDist} for p-distance, i.e., the same as edit distance, with positions with gaps or missing values discarded, {\tt JC} for Jukes-Cantor distance, {\tt Kimura} for Kimura distance, and {\tt AminoKimura} for Kimura distance between amino acid sequences. See the Phylip documentation \cite{Phylip3.6-08:misc} for details.
\end{itemize}

\section{Output Files}
De \emph{filename.out} file returned by \clusphy{} can be postprocessed by the \emph{postprocess\_tree.pl} perl script in the {\tt data/phylo/} directory. This script returns two files: \emph{filename.clus-phy\_tree}, which contains two tree representations (one with just the topology, and one with all mutations listed), and \emph{filename.clus-phy\_newick}, which contains the tree in the so-called Newick format, where clusters are represented by pairs of parentheses.


\bibliographystyle{plain}
\bibliography{clus-manual}

\end{document}
